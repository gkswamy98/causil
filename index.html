<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<link href="https://fonts.googleapis.com/css2?family=Lato&display=swap"
      rel="stylesheet">
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
	<title>What would the Expert do()?: Causal Imitation Learning</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
	<meta property="og:title" content="Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap" />
	<meta property="og:description" content="When attempting to mimic an expert, a learner could learn by (a) rolling out their policy and comparing generated trajectories to expert trajectories, (b) producing actions on expert states and attempting to match action-conditionals, or (c) performing rollouts and attempting to match corrections provided by a queryable expert. We provide, for each of these settings, bounds for how well the learner can do, reduction-based algorithms for efficiently finding strong policies, and simple yet competitive practical instantiations that can scale to high-dimensional tasks." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="When attempting to mimic an expert, a learner could learn by (a) rolling out their policy and comparing generated trajectories to expert trajectories, (b) producing actions on expert states and attempting to match action-conditionals, or (c) performing rollouts and attempting to match corrections provided by a queryable expert. We provide, for each of these settings, bounds for how well the learner can do, reduction-based algorithms for efficiently finding strong policies, and simple yet competitive practical instantiations that can scale to high-dimensional tasks." />
    <meta property="twitter:title"         content="Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap" />
    <meta property="twitter:description"   content="When attempting to mimic an expert, a learner could learn by (a) rolling out their policy and comparing generated trajectories to expert trajectories, (b) producing actions on expert states and attempting to match action-conditionals, or (c) performing rollouts and attempting to match corrections provided by a queryable expert. We provide, for each of these settings, bounds for how well the learner can do, reduction-based algorithms for efficiently finding strong policies, and simple yet competitive practical instantiations that can scale to high-dimensional tasks." />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

</head>

<body>
<div class="container">
    <div class="title">
        What would the Expert $do(\cdot)$?: <br> Causal Imitation Learning
    </div>

    <div class="venue">
        Oral at NeurIPS'21 Safe and Robust Control of Uncertain Systems and Offline RL Workshops
    </div>

    <br><br>

    <div class="author">
        <a href="https://gokul.dev/">Gokul Swamy</a><sup>1</sup>
    </div>
    <div class="author">
        <a href="http://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://www.ri.cmu.edu/ri-faculty/j-andrew-drew-bagnell/">Drew Bagnell</a><sup>1, 2</sup>
    </div>
    <div class="author">
        <a href="https://zstevenwu.com/">Steven Wu</a><sup>3</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>Robotics Institute, CMU</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Aurora Innovation</div>
    <div class="affiliation"><sup>3&nbsp;</sup>ISR, CMU</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2103.03236"><i class="fa fa-file-text", style="font-size: 50px; padding-bottom: 10px"></i><br>[Paper]</a></div>
    <div class="links"><a href="https://www.youtube.com/playlist?list=PL51kEpt5uSsbZSaGyUMsLsOoFP8-hyx0R"><i class="fa fa-play-circle" style="font-size: 50px; padding-bottom: 10px"></i><br>[Videos]</a></div>
    <div class="links"><a href="https://github.com/gkswamy98/causal_il"><i class="fa fa-github" style="font-size: 50px; padding-bottom: 10px"></i><br>[Code]</a></div>

    <br><br>

    <img style="width: 60%;" src="./resources/causil_ffig.svg" alt="Teaser figure."/>
    <br>
    <p style="width: 80%;">
      <br>
  We focus in imitation learning in the presence of temporally correlated perturbations (exogenous noise, <i>(a)</i>) or not having access to the full state (endogenous noise, <i>(b)<i>). We formalize both in a graphical model <i>(c)</i> that allows us to leverage a technique known as <i>instrumental variable regression</i> to find a policy that isn't corrupted by spurious correlations introduced by the confounder.
    <br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
We develop algorithms for imitation learning from policy data that was corrupted by unobserved confounders. Sources of such confounding include <i>(a)</i> persistent perturbations to actions or <i>(b)</i> the expert responding to a part of the state that the learner does not have access to. When a confounder affects multiple timesteps of recorded data, it can manifest as spurious correlations between states and actions that a learner might latch on to, leading to poor policy performance. To break up these spurious correlations, we apply modern variants of the classical <i>instrumental variable regression</i> (IVR) technique, enabling us to recover the causally correct underlying policy \textit{without} requiring access to an interactive expert. In particular, we present two techniques, one of a generative-modeling flavor (<code>DoubIL</code>) that can utilize access to a simulator and one of a game-theoretic flavor (<code>ResiduIL</code>) that can be run entirely offline. We then prove performance bounds for both algorithms, discuss how they scale with the strength of the instrument, and discuss situations where effective imitation is not possible. We find both of our algorithms compare favorably to behavioral cloning on simulated rocket landing task.    </p>
    </p>

    <br>
    <hr>

    <h1>Videos</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/srhPFS77oSs" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/zczKcpzwRuc" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>
    <br>

    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org/abs/2103.03236">
            <img class="layered-paper-big" width="100%" src="./resources/paper.svg" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>What would the Expert do()?: Causal Imitation Learning</h3>
        <p>Gokul Swamy, Sanjiban Choudhury, J. Andrew Bagnell, Zhiwei Steven Wu</p>
        <p>NeurIPS Safe and Robust Control of Uncertain Systems and Offline RL Workshops, 2021.</p>
        <pre><code>@InProceedings{swamy2021moments,
    title = {Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap},
    author = {Gokul Swamy and Sanjiban Choudhury and J. Andrew Bagnell and Zhiwei Steven Wu},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning},
    year = {2021},
}</code></pre>
    </div>

    <br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
        and <a href="http://richzhang.github.io/">Richard Zhang</a> for a
        <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project, and
        adapted to be mobile responsive by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code we built on can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p>

    <br>
</div>

</body>

</html>
